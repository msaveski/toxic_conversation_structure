{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../processing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from _config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> news\n",
      "|news| = 193040\n",
      ">> midterms\n",
      "|midterms| = 100286\n"
     ]
    }
   ],
   "source": [
    "ds_names = [\"news\", \"midterms\"]\n",
    "\n",
    "feature_set = set()\n",
    "feature_types = defaultdict(set)\n",
    "all_types = set()\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print(f\">> {ds_name}\")\n",
    "    \n",
    "    # load dataset\n",
    "    conf = Config(ds_name)\n",
    "    ds_path = f\"{conf.data_root}/next_reply_metrics/{ds_name}_paired_sample.json.gz\"\n",
    "\n",
    "    with gzip.open(ds_path) as fin:\n",
    "        ds = json.load(fin)\n",
    "\n",
    "    print(f\"|{ds_name}| = {len(ds)}\")\n",
    "    \n",
    "    # loop through all conversations\n",
    "    for conv in ds:\n",
    "        for f_name, f_val in conv.items():\n",
    "            feature_set.add(f_name)\n",
    "            feature_types[f_name].add(str(type(f_val)))\n",
    "            all_types.add(str(type(f_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(feature_set) = 270\n"
     ]
    }
   ],
   "source": [
    "print(\"len(feature_set) =\", len(feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'NoneType'>\",\n",
       " \"<class 'bool'>\",\n",
       " \"<class 'float'>\",\n",
       " \"<class 'int'>\",\n",
       " \"<class 'str'>\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id {\"<class 'str'>\"}\n",
      "root_tweet_id {\"<class 'str'>\"}\n",
      "root_tweet_type {\"<class 'str'>\"}\n",
      "dyad_up_follow_edge_type {\"<class 'str'>\", \"<class 'NoneType'>\"}\n",
      "dyad_ur_follow_edge_type {\"<class 'str'>\", \"<class 'NoneType'>\"}\n"
     ]
    }
   ],
   "source": [
    "# SUMMARY\n",
    "# meta:\n",
    "# tweet_id, root_tweet_id, root_tweet_type\n",
    "#\n",
    "# categorical features: \n",
    "# dyad_up_follow_edge_type, dyad_ur_follow_edge_type\n",
    "\n",
    "for f_fname, f_types in feature_types.items():\n",
    "    if \"<class 'str'>\" in f_types:\n",
    "        print(f_fname, f_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `feature_sets.json`\n",
    "- manually created based on the output here\n",
    "- includes the heierarcy of features\n",
    "- includes the values of the categorical features (i.e., dyad edge types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_to_one_hot(features):\n",
    "    # make a copy, instead of changing the features in-place\n",
    "    features_ = features.copy()\n",
    "    \n",
    "    # Only one categorical feature: dyad-edge-type\n",
    "    dyad_etypes_map = [\n",
    "        (None, \"na\"),\n",
    "        (\"O==O\", \"O==O\"), \n",
    "        (\"O->O\", \"O->O\"), \n",
    "        (\"O<-O\", \"O<-O\"),\n",
    "        (\"O  O\", \"O__O\")\n",
    "    ]\n",
    "    \n",
    "    # user-parent-dyad & user-root-dyad\n",
    "    dyad_fnames = [\n",
    "        \"dyad_up_follow_edge_type\",\n",
    "        \"dyad_ur_follow_edge_type\"\n",
    "    ]\n",
    "    \n",
    "    for f_name in dyad_fnames:\n",
    "        f_val_sum = 0\n",
    "        \n",
    "        for dyad_etype_val, dyad_etype_str in dyad_etypes_map:\n",
    "            f_key = f\"{f_name}_{dyad_etype_str}\"\n",
    "            f_val = int(features_[f_name] == dyad_etype_val)\n",
    "            features_[f_key] = f_val\n",
    "            f_val_sum += f_val\n",
    "            \n",
    "        # sanity check: exactly one value is on\n",
    "        assert f_val_sum == 1\n",
    "        \n",
    "        # remove the string feature\n",
    "        del features_[f_name]\n",
    "    \n",
    "    return features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_matrix(dataset, feature_set_name_pairs):\n",
    "    X, y, meta = [], [], []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        conv = dataset[i]\n",
    "        \n",
    "        # features\n",
    "        x_i = []\n",
    "\n",
    "        # make categorical features 1-hot\n",
    "        conv = categorical_to_one_hot(conv)\n",
    "\n",
    "        for f_set, f_name in feature_set_name_pairs:\n",
    "            # NB: only dyad subgraph metrics are missing\n",
    "            f_val = conv.get(f_name, None)\n",
    "            \n",
    "            # encode missing values as np.nans => np array is of type float\n",
    "            f_val = np.nan if f_val is None else f_val\n",
    "            f_val = float(f_val)\n",
    "            \n",
    "            x_i.append(f_val)\n",
    "\n",
    "        X.append(x_i)\n",
    "\n",
    "        # outcome\n",
    "        y_i = float(conv[\"tweet_tox\"])\n",
    "        y.append(y_i)\n",
    "        \n",
    "        # meta\n",
    "        meta.append({\n",
    "            \"tweet_id\": conv[\"tweet_id\"],\n",
    "            \"root_tweet_id\": conv[\"root_tweet_id\"],\n",
    "            \"root_tweet_type\": conv[\"root_tweet_type\"],\n",
    "            \"n_replies\": conv[\"conv_n_replies\"],\n",
    "            \"tox_score\": conv[\"tweet_tox_score\"]\n",
    "        })\n",
    "    \n",
    "    X_arr = np.array(X)\n",
    "    y_arr = np.array(y)\n",
    "\n",
    "    # sanity checks\n",
    "    assert X_arr.shape[0] == y_arr.shape[0] == len(meta)\n",
    "    assert X_arr.shape[1] == len(feature_set_name_pairs)\n",
    "    assert str(X_arr.dtype) == \"float64\"\n",
    "    assert str(y_arr.dtype) == \"float64\"\n",
    "\n",
    "    out = {\n",
    "        \"X\": X_arr,\n",
    "        \"y\": y_arr,\n",
    "        \"meta\": meta,\n",
    "        \"feature_set_name_pairs\": feature_set_name_pairs\n",
    "    }\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature sets\n",
    "conf = Config()\n",
    "feature_sets_fpath = f\"{conf.modeling_dir}/next_reply/feature_sets.json\"\n",
    "feature_sets = json.load(open(feature_sets_fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- outcome 1\n",
      "- meta 4\n",
      "- conversation_state 9\n",
      "- user_info 3\n",
      "- alignments 2\n",
      "- follow_di 15\n",
      "- follow_ud 12\n",
      "- reply_di 15\n",
      "- reply_ud 12\n",
      "- dyad_up 50\n",
      "- dyad_ur 20\n",
      "- embeddedness_all 10\n",
      "- embeddedness_toxicity 20\n",
      "- embeddedness_follow 50\n",
      "- embeddedness_reply 50\n",
      "- tree 5\n"
     ]
    }
   ],
   "source": [
    "for fset_name, features in feature_sets.items():\n",
    "    print(\"-\", fset_name, len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of features: 273\n"
     ]
    }
   ],
   "source": [
    "# make feature set, name pairs\n",
    "feature_set_name_pairs = []\n",
    "\n",
    "for f_set_name, f_set_features in feature_sets.items():\n",
    "    if f_set_name in (\"meta\", \"outcome\"):\n",
    "        continue\n",
    "    for f_name in f_set_features:\n",
    "        feature_set_name_pairs.append((f_set_name, f_name))\n",
    "\n",
    "print(f\"Total num of features: {len(feature_set_name_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> news\n",
      "|news| = 193040\n",
      "dict_keys(['X', 'y', 'meta', 'feature_set_name_pairs']) (193040, 273)\n",
      ">> midterms\n",
      "|midterms| = 100286\n",
      "dict_keys(['X', 'y', 'meta', 'feature_set_name_pairs']) (100286, 273)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# create & output all data matrices\n",
    "ds_names = [\"news\", \"midterms\"]\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print(f\">> {ds_name}\")\n",
    "    \n",
    "    conf = Config(ds_name)\n",
    "    \n",
    "    # load dataset\n",
    "    ds_path = f\"{conf.data_root}/next_reply_metrics/{ds_name}_paired_sample.json.gz\"\n",
    "    ds = json.load(gzip.open(ds_path))    \n",
    "    print(f\"|{ds_name}| = {len(ds)}\")\n",
    "    \n",
    "    # make dataset matrix\n",
    "    ds_mat = make_dataset_matrix(ds, feature_set_name_pairs)\n",
    "    print(ds_mat.keys(), ds_mat[\"X\"].shape)\n",
    "\n",
    "    # output ds matrix\n",
    "    out_path = f\"{conf.modeling_dir}/next_reply/datasets/{ds_name}_paired.pkl.gz\"\n",
    "\n",
    "    with gzip.open(out_path, \"wb\") as fout:\n",
    "        pickle.dump(ds_mat, fout, protocol=4)\n",
    "        \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('dtox': virtualenv)",
   "language": "python",
   "name": "python37264bitdtoxvirtualenvbf1c2042b6b64ee8b97dfe3287fe0a24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
