{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../processing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import treelib\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from _config import Config\n",
    "from utils import json_paths_iter\n",
    "from tree_conversions import get_tree_tweet_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"news\"\n",
    "limit = None\n",
    "\n",
    "conf = Config(dataset)\n",
    "json_fpaths = json_paths_iter(conf.conversations_no_embs_jsons_dir, limit=limit)\n",
    "\n",
    "# counts\n",
    "root_tox_counts = {}  # root_t_id => {n_pos: x, n_neg: y}\n",
    "tox_counts = {\"pos\": 0, \"neg\": 0}\n",
    "\n",
    "for json_fpath in tqdm(json_fpaths):\n",
    "\n",
    "    conversation = json.load(gzip.open(json_fpath))\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    tweet_info = conversation[\"tweets\"]\n",
    "    net_info = conversation[\"network_features\"]\n",
    "    toxicity_scores = conversation[\"toxicity_scores\"]\n",
    "    \n",
    "    # root info\n",
    "    root_tweet_id = conversation[\"reply_tree\"][\"tweet\"]\n",
    "    root_user_id = conversation[\"tweets\"][root_tweet_id][\"user_id\"]\n",
    "    \n",
    "    # tweets in chronological order\n",
    "    tweets = list(conversation[\"tweets\"].values())\n",
    "    tweets.sort(key=lambda x: x[\"time\"])\n",
    "    \n",
    "    # get inReplyTo links (child_tweet_id => parent_tweet_id)\n",
    "    tweet_replyto_id = get_tree_tweet_edges(conversation[\"reply_tree\"])\n",
    "    tweet_replyto_id = {c_t_id: p_t_id for p_t_id, c_t_id in tweet_replyto_id}\n",
    "    tweet_replyto_id[root_tweet_id] = None\n",
    "    \n",
    "    # user id (str) => idx\n",
    "    user_id_to_idx = enumerate(net_info[\"user_ids\"])\n",
    "    user_id_to_idx = {u_id: u_idx for u_idx, u_id in user_id_to_idx}\n",
    "    \n",
    "    # init tree\n",
    "    tree = treelib.Tree()\n",
    "    \n",
    "    conv_tox_counts = {\"pos\": [], \"neg\": []}\n",
    "\n",
    "    # LOOP\n",
    "    for tweet in tweets:\n",
    "        # tweet / user\n",
    "        tweet_id = tweet[\"id\"]\n",
    "        user_id = tweet[\"user_id\"]\n",
    "        user_idx = user_id_to_idx.get(user_id, None)\n",
    "        \n",
    "        tweet_tox_score = toxicity_scores.get(tweet_id, None)\n",
    "        tweet_tox_ok = False\n",
    "        if tweet_tox_score is not None and (tweet_tox_score < 0.25 or tweet_tox_score > 0.75):\n",
    "            tweet_tox_ok = True\n",
    "\n",
    "        # parent\n",
    "        parent_tweet_id = tweet_replyto_id[tweet_id]\n",
    "        parent_user_id = None\n",
    "        if parent_tweet_id is not None:\n",
    "            parent_user_id = tweet_info[parent_tweet_id][\"user_id\"]\n",
    "        \n",
    "        # update reply tree\n",
    "        tree.create_node(\n",
    "            identifier=tweet_id, \n",
    "            parent=tweet_replyto_id[tweet_id]\n",
    "        )\n",
    "        \n",
    "        # FILTER\n",
    "        SKIP = (\n",
    "            user_id == root_user_id or          # tweets by the root\n",
    "            user_id == parent_user_id or        # self-replies\n",
    "            user_id not in user_id_to_idx or    # users w/o follow net info\n",
    "            tweet_tox_ok == False or            # tweets w/o toxicity label\n",
    "            tree.depth(tweet_id) < 2            # direct replies to the root\n",
    "        )\n",
    "        \n",
    "        # SAMPLE        \n",
    "        if not SKIP:\n",
    "            # counts code here\n",
    "            tweet_tox_label = \"pos\" if tweet_tox_score > 0.5 else \"neg\"\n",
    "            \n",
    "            conv_tox_counts[tweet_tox_label].append(tweet_id)\n",
    "            tox_counts[tweet_tox_label] += 1\n",
    "            \n",
    "    # save counts\n",
    "    json_fname = json_fpath.split(\"/\")[-1]\n",
    "    root_tox_counts[(json_fname, root_tweet_id)] = conv_tox_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tox_counts, sum(tox_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [True for v in root_tox_counts.values() if len(v[\"pos\"]) > 0 and len(v[\"neg\"]) > 0]\n",
    "print(len(cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to pickle\n",
    "out = {\n",
    "    \"tox_counts\": tox_counts,\n",
    "    \"root_tox_counts\": root_tox_counts\n",
    "}\n",
    "\n",
    "out_fpath = f\"{conf.data_root}/next_reply_metrics/{dataset}_tweets_tox_p75_m25.pkl.gz\"\n",
    "\n",
    "with gzip.open(out_fpath, \"wb\") as fout:\n",
    "    pickle.dump(out, fout, protocol=4)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news\n",
    "# {'pos': 879,165, 'neg': 4,927,127} 5,806,292\n",
    "# 96,520\n",
    "\n",
    "# midterms\n",
    "# {'pos': 641,494, 'neg': 4,362,548} 5,004,042\n",
    "# 50,143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Tweets Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset = \"news\"\n",
    "\n",
    "conf = Config(dataset)\n",
    "fpath = f\"{conf.data_root}/next_reply_metrics/{dataset}_tweets_tox_p75_m25.pkl.gz\"\n",
    "\n",
    "tox_stats = pickle.load(gzip.open(fpath))\n",
    "\n",
    "ds_tox_counts = tox_stats[\"tox_counts\"]\n",
    "conv_tox_tweets = tox_stats[\"root_tox_counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tox_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample one toxic and one non-toxic tweet from each conversation\n",
    "paired_tweets = []  # (fname, root_id, tweet_ids)\n",
    "RNG = random.Random(0)\n",
    "\n",
    "for file_root_id_pair, tweet_tox in conv_tox_tweets.items():\n",
    "    fname, root_id = file_root_id_pair\n",
    "    pos_t_ids = tweet_tox[\"pos\"]\n",
    "    neg_t_ids = tweet_tox[\"neg\"]\n",
    "    \n",
    "    if len(pos_t_ids) < 1 or len(neg_t_ids) < 1:\n",
    "        continue\n",
    "    \n",
    "    tweets_pair = [RNG.choice(pos_t_ids), RNG.choice(neg_t_ids)]\n",
    "    \n",
    "    paired_tweets.append((fname, root_id, tweets_pair))\n",
    "    \n",
    "print(len(paired_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "fnames_unq = set()\n",
    "r_ids_unq = set()\n",
    "t_ids_unq = set()\n",
    "\n",
    "for fname, r_id, t_ids in paired_tweets:\n",
    "    fnames_unq.add(fname)\n",
    "    r_ids_unq.add(r_id)\n",
    "    for t_id in t_ids:\n",
    "        t_ids_unq.add(t_id)\n",
    "\n",
    "assert len(paired_tweets) == len(fnames_unq) == len(r_ids_unq)\n",
    "assert len(t_ids_unq) == len(paired_tweets) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to file \n",
    "out_path = f\"{conf.data_root}/next_reply_metrics/{dataset}_paired_sample_tweet_ids.json.gz\"\n",
    "\n",
    "with gzip.open(out_path, \"wt\") as fout:\n",
    "    json.dump(paired_tweets, fout, indent=2)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('dtox': virtualenv)",
   "language": "python",
   "name": "python37264bitdtoxvirtualenvbf1c2042b6b64ee8b97dfe3287fe0a24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
