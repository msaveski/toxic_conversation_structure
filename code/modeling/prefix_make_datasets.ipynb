{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../processing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from _config import Config\n",
    "\n",
    "# NB: ujson has trouble loading the jsons for some reason | ujson issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_names = [\"midterms\", \"news\"]\n",
    "prefixes = [str(p) for p in range(10, 110, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_feature_groups(conv):\n",
    "    # remove redundant features\n",
    "    rm_group_fname_tuples = [\n",
    "        ('tree', 'root_tweet_id'),\n",
    "        ('follow_graph', 'root_tweet_id'),\n",
    "        ('reply_graph', 'root_tweet_id'),\n",
    "        ('polarization', 'root_tweet_id'),\n",
    "        ('embeddedness', 'root_tweet_id'),\n",
    "        ('subgraph', 'root_tweet_id'),\n",
    "        (\"polarization\", \"follow_di_n_nodes\"),\n",
    "        (\"polarization\", \"follow_ud_n_nodes\"),\n",
    "        (\"polarization\", \"reply_di_n_nodes\"),\n",
    "        (\"polarization\", \"reply_ud_n_nodes\")\n",
    "    ]\n",
    "    \n",
    "    for group, fname in rm_group_fname_tuples:\n",
    "        if conv[group] is not None:\n",
    "            del conv[group][fname]\n",
    "    \n",
    "    # NB: rearrange a few features\n",
    "    # (Makes more sense to group them differently while computing them)\n",
    "    mv_group_from_to_tuples = [\n",
    "        # polarization -> follow_graph\n",
    "        (\"follow_di_alg_cat_corr\", \"polarization\", \"follow_graph\"),\n",
    "        (\"follow_di_alg_num_corr\", \"polarization\", \"follow_graph\"),\n",
    "        (\"follow_ud_alg_cat_corr\", \"polarization\", \"follow_graph\"),\n",
    "        (\"follow_ud_alg_modularity\", \"polarization\", \"follow_graph\"),\n",
    "        (\"follow_ud_alg_num_corr\", \"polarization\", \"follow_graph\"),\n",
    "        # polarization -> reply_graph\n",
    "        (\"reply_di_alg_cat_corr\", \"polarization\", \"reply_graph\"), \n",
    "        (\"reply_di_alg_num_corr\", \"polarization\", \"reply_graph\"), \n",
    "        (\"reply_ud_alg_cat_corr\", \"polarization\", \"reply_graph\"), \n",
    "        (\"reply_ud_alg_modularity\", \"polarization\", \"reply_graph\"), \n",
    "        (\"reply_ud_alg_num_corr\", \"polarization\", \"reply_graph\"), \n",
    "        # polarization -> tree\n",
    "        (\"tree_alg_cat_corr\", \"polarization\", \"tree\"), \n",
    "        (\"tree_alg_num_corr\", \"polarization\", \"tree\"),\n",
    "    ]\n",
    "    \n",
    "    for fname, g_from, g_to in mv_group_from_to_tuples:\n",
    "        if conv[g_from] is not None and conv[g_to] is not None:\n",
    "            val = conv[g_from][fname]\n",
    "            conv[g_to][fname] = val\n",
    "        \n",
    "        if conv[g_from] is not None:\n",
    "            del conv[g_from][fname]\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> midterms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130931it [07:53, 276.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182365it [17:55, 169.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# compile all features\n",
    "feature_sets = defaultdict(set)\n",
    "feature_types = defaultdict(set)\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print(f\">> {ds_name}\")\n",
    "    conf = Config(ds_name)\n",
    "    ds_path = f\"{conf.data_root}/prefix_metrics/{ds_name}.json.gz\"\n",
    "\n",
    "    with gzip.open(ds_path) as fin:\n",
    "        ds = json.load(fin)\n",
    "\n",
    "    for conv_idx, conv in tqdm(enumerate(ds)):\n",
    "        \n",
    "        for ps in prefixes:\n",
    "            if ps not in conv:\n",
    "                continue\n",
    "            \n",
    "            conv_ps = conv[ps]\n",
    "            conv_ps = rearrange_feature_groups(conv_ps)\n",
    "            \n",
    "            for f_set, fs in conv_ps.items():\n",
    "                if fs is None:\n",
    "                    continue\n",
    "\n",
    "                for f_name, f_val in fs.items():\n",
    "                    feature_sets[f_set].add(f_name)\n",
    "                    feature_types[(f_set, f_name)].add(str(type(f_val)))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting feature sets to file\n",
    "conf = Config()\n",
    "feature_set_fpath = f\"{conf.modeling_dir}/prefix/feature_sets.json\"\n",
    "\n",
    "feature_sets_lst = {fset_name: sorted(list(fset)) \\\n",
    "                    for fset_name, fset in feature_sets.items()}\n",
    "\n",
    "with open(feature_set_fpath, \"w\") as fout:\n",
    "    json.dump(feature_sets_lst, fout, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_matrix(dataset, prefix, feature_set_name_pairs):\n",
    "    root_tweet_ids = []\n",
    "    X = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        \n",
    "        if prefix not in dataset[i]:\n",
    "            continue\n",
    "            \n",
    "        conv = dataset[i][prefix]\n",
    "        conv = rearrange_feature_groups(conv)\n",
    "        root_tweet_id = dataset[i][\"root_tweet_id\"]\n",
    "        x_i = []\n",
    "\n",
    "        for f_set, f_name in feature_set_name_pairs:\n",
    "            # NB: feature subset might be missing\n",
    "            f_set_dict = conv[f_set] if conv[f_set] is not None else {}\n",
    "            \n",
    "            # feature might be missing\n",
    "            f_val = f_set_dict.get(f_name, None)\n",
    "            \n",
    "            # encode missing values as np.nans => np array is of type float\n",
    "            f_val = f_val if f_val is not None else np.nan\n",
    "            f_val = float(f_val)\n",
    "            \n",
    "            x_i.append(f_val)\n",
    "        \n",
    "        X.append(x_i)\n",
    "        root_tweet_ids.append(root_tweet_id)\n",
    "\n",
    "    X_arr = np.array(X)\n",
    "    assert X_arr.shape[0] == len(root_tweet_ids)\n",
    "    assert X_arr.shape[1] == len(feature_set_name_pairs)\n",
    "    assert str(X_arr.dtype) == \"float64\"\n",
    "        \n",
    "    out = {\n",
    "        \"X\": X_arr,\n",
    "        \"root_tweet_ids\": root_tweet_ids,\n",
    "        \"feature_set_name_pairs\": feature_set_name_pairs\n",
    "    }\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- tree 46\n",
      "- follow_graph 115\n",
      "- reply_graph 115\n",
      "- embeddedness 56\n",
      "- polarization 12\n",
      "- subgraph 60\n",
      "- arrival_seq 198\n",
      "- rate 200\n",
      "- toxicity 7\n",
      "[Total num of features: 809]\n"
     ]
    }
   ],
   "source": [
    "# load feature sets\n",
    "fsets_fpath = f\"{Config().modeling_dir}/prefix/feature_sets.json\"\n",
    "feature_sets = json.load(open(fsets_fpath))\n",
    "    \n",
    "# make feature set, name pairs\n",
    "feature_set_name_pairs = []\n",
    "\n",
    "for f_set_name, f_set_features in feature_sets.items():\n",
    "    for f_name in f_set_features:\n",
    "        feature_set_name_pairs.append((f_set_name, f_name))\n",
    "    \n",
    "    print(\"-\", f_set_name, len(f_set_features))\n",
    "    \n",
    "print(f\"[Total num of features: {len(feature_set_name_pairs)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> midterms\n",
      "prefix: 10\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (130931, 809)\n",
      "prefix: 20\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (81970, 809)\n",
      "prefix: 30\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (61900, 809)\n",
      "prefix: 40\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (50840, 809)\n",
      "prefix: 50\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (43627, 809)\n",
      "prefix: 60\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (38440, 809)\n",
      "prefix: 70\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (34598, 809)\n",
      "prefix: 80\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (31516, 809)\n",
      "prefix: 90\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (28966, 809)\n",
      "prefix: 100\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (26896, 809)\n",
      "-----\n",
      ">> news\n",
      "prefix: 10\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (182365, 809)\n",
      "prefix: 20\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (147702, 809)\n",
      "prefix: 30\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (127040, 809)\n",
      "prefix: 40\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (111943, 809)\n",
      "prefix: 50\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (100173, 809)\n",
      "prefix: 60\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (90709, 809)\n",
      "prefix: 70\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (83020, 809)\n",
      "prefix: 80\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (76368, 809)\n",
      "prefix: 90\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (70688, 809)\n",
      "prefix: 100\n",
      "dict_keys(['X', 'root_tweet_ids', 'feature_set_name_pairs']) (65884, 809)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# create & output all data matrices | dataset x prefix_size\n",
    "\n",
    "ds_names = [\"midterms\", \"news\"]\n",
    "prefixes = [str(p) for p in range(10, 110, 10)]\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print(f\">> {ds_name}\")\n",
    "    \n",
    "    # load dataset \n",
    "    conf = Config(ds_name)\n",
    "    ds_path = f\"{conf.data_root}/prefix_metrics/{ds_name}.json.gz\"\n",
    "\n",
    "    with gzip.open(ds_path) as fin:\n",
    "        ds = json.load(fin)\n",
    "        \n",
    "    # make matrix for every prefix\n",
    "    for prefix in prefixes:\n",
    "        print(f\"prefix: {prefix}\")\n",
    "        \n",
    "        ds_mat = make_dataset_matrix(ds, prefix, feature_set_name_pairs)\n",
    "        print(ds_mat.keys(), ds_mat[\"X\"].shape)\n",
    "        \n",
    "        # output to file\n",
    "        out_path = f\"{conf.modeling_dir}/prefix/datasets/{ds_name}_p{prefix}.pkl.gz\"\n",
    "\n",
    "        with gzip.open(out_path, \"wb\") as fout:\n",
    "            pickle.dump(ds_mat, fout, protocol=4)\n",
    "            \n",
    "    print(\"-----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('dtox': virtualenv)",
   "language": "python",
   "name": "python37264bitdtoxvirtualenvbf1c2042b6b64ee8b97dfe3287fe0a24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
