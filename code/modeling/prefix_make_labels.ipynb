{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../processing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from _config import Config\n",
    "from utils import json_paths_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> news\n",
      "0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000 110000 120000 130000 140000 150000 160000 170000 180000 190000 200000 210000 220000 230000 240000 250000 260000 270000 280000 290000 300000 310000 320000 330000 340000 350000 360000 370000 380000 390000 400000 410000 420000 430000 440000 450000 460000 470000 480000 490000 500000 510000 \n",
      ">> midterms\n",
      "0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000 110000 120000 130000 140000 150000 160000 170000 180000 190000 200000 210000 220000 230000 240000 250000 260000 270000 280000 290000 300000 310000 320000 330000 340000 350000 360000 370000 380000 390000 400000 410000 420000 430000 440000 450000 460000 470000 480000 490000 500000 510000 520000 530000 540000 550000 560000 570000 580000 590000 600000 610000 620000 630000 640000 650000 660000 670000 "
     ]
    }
   ],
   "source": [
    "ds_names = [\"news\", \"midterms\"]\n",
    "prefixes = [p for p in range(10, 110, 10)]\n",
    "TOX_THRESHOLD = 0.531\n",
    "ds_map = {}\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print(\"\\n>>\", ds_name)\n",
    "\n",
    "    ds = {}\n",
    "    conf = Config(ds_name)\n",
    "    fpaths = json_paths_iter(conf.conversations_no_embs_jsons_dir)\n",
    "    \n",
    "    for i, fpath in enumerate(fpaths):\n",
    "        if i % 10000 == 0: print(i, end=\" \")\n",
    "\n",
    "        conversation = json.load(gzip.open(fpath))\n",
    "        \n",
    "        root_tweet_id = conversation[\"reply_tree\"][\"tweet\"]\n",
    "        \n",
    "        # sort tweets\n",
    "        tweets = list(conversation[\"tweets\"].values())\n",
    "        tweets.sort(key=lambda x: x[\"time\"])\n",
    "\n",
    "        # id_str => tox score\n",
    "        tox_scores = conversation[\"toxicity_scores\"]\n",
    "\n",
    "        # tox_scores array => 1 index\n",
    "        N = len(tweets)\n",
    "        tox_scores_arr = np.zeros(N + 1)\n",
    "\n",
    "        # dummy value: number to avoid comparison with none\n",
    "        tox_scores_arr[0] = -100\n",
    "        for t_idx, tweet in enumerate(tweets):\n",
    "            tox_scores_arr[t_idx + 1] = tox_scores.get(tweet[\"id\"], -1)\n",
    "\n",
    "        tox_scores_bin = tox_scores_arr > TOX_THRESHOLD\n",
    "        tox_scores_cum_sum = np.cumsum(tox_scores_bin, dtype=float)\n",
    "        tox_total = tox_scores_cum_sum[-1]\n",
    "\n",
    "        assert np.sum(tox_scores_bin) == tox_scores_cum_sum[-1]\n",
    "        \n",
    "        # tweet data\n",
    "        tweet_dict = {}\n",
    "        tweet_dict[\"root_tweet_id\"] = root_tweet_id\n",
    "        tweet_dict[\"root_tweet_type\"] = conversation[\"root_tweet_type\"]\n",
    "        tweet_dict[\"n\"] = N\n",
    "        \n",
    "        for pre_n in prefixes:\n",
    "            if 2 * pre_n <= N:\n",
    "                # [1, prefix]\n",
    "                pre_n_tox = tox_scores_cum_sum[pre_n]\n",
    "\n",
    "                # (prefix, suffix]\n",
    "                suf_i_tox = tox_scores_cum_sum[2 * pre_n] - pre_n_tox\n",
    "\n",
    "                # (prefix, end]\n",
    "                suf_n = N - pre_n\n",
    "                suf_n_tox = tox_total - pre_n_tox\n",
    "                suf_f_tox = suf_n_tox / suf_n\n",
    "\n",
    "                tweet_dict[f\"p{pre_n}_pre_n_tox\"] = pre_n_tox\n",
    "                tweet_dict[f\"p{pre_n}_suf_i_tox\"] = suf_i_tox\n",
    "                tweet_dict[f\"p{pre_n}_suf_n\"] = suf_n\n",
    "                tweet_dict[f\"p{pre_n}_suf_n_tox\"] = suf_n_tox\n",
    "                tweet_dict[f\"p{pre_n}_suf_f_tox\"] = suf_f_tox\n",
    "        \n",
    "        # save tweet data\n",
    "        ds[root_tweet_id] = tweet_dict\n",
    "        \n",
    "    # write ds to ds_map\n",
    "    ds_map[ds_name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_map_fpath = f\"{Config().root}/data/modeling/prefix/label_maps.pkl.gz\"\n",
    "with gzip.open(ds_map_fpath, \"wb\") as fout:\n",
    "    pickle.dump(ds_map, fout, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_map_fpath = f\"{Config().root}/data/modeling/prefix/label_maps.pkl.gz\"\n",
    "ds_map = pickle.load(gzip.open(ds_map_fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"midterms\"\n",
    "ds = ds_map[ds_name]\n",
    "prefixes = [p for p in range(10, 110, 10)]\n",
    "metrics = [\"suf_i_tox\", \"suf_f_tox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1] for each prefix & metric collect all root_tweet_id, values pairs\n",
    "\n",
    "acc = defaultdict(list) \n",
    "\n",
    "for conv in ds.values():    \n",
    "    for p in prefixes:\n",
    "        for metric in metrics:\n",
    "            if f\"p{p}_pre_n_tox\" not in conv:\n",
    "                continue\n",
    "\n",
    "            pre_n_tox = int(conv[f\"p{p}_pre_n_tox\"])\n",
    "            id_val_pair = (conv[\"root_tweet_id\"], conv[f\"p{p}_{metric}\"])\n",
    "\n",
    "            acc[f\"p{p}__{metric}__all\"].append(id_val_pair)\n",
    "            acc[f\"p{p}__{metric}__pre_n_tox_{pre_n_tox}\"].append(id_val_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2] for each parition metrics\n",
    "# --- compute the aggregate stats\n",
    "# --- split the examples\n",
    "# --- downsample to get class balance\n",
    "\n",
    "def balance_samples(x, y, RNG):\n",
    "    # downsample s.t. |x| == |y|\n",
    "    if len(x) > len(y):\n",
    "        x = RNG.sample(x, len(y))\n",
    "    elif len(y) > len(x):\n",
    "        y = RNG.sample(y, len(x))\n",
    "\n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "paritions = {}\n",
    "min_bucket_size = 200\n",
    "agg_stats = {\"n\": {}, \"q33\": {}, \"q50\": {}, \"q66\": {}}\n",
    "\n",
    "for key, root_id_val_pairs in acc.items():\n",
    "    \n",
    "    vals = [val for _, val in root_id_val_pairs]\n",
    "    \n",
    "    q33, q50, q66 = np.quantile(vals, [1/3, 0.5, 2/3])\n",
    "    \n",
    "    # save aggregates\n",
    "    agg_stats[\"n\"][key] = len(vals)\n",
    "    agg_stats[\"q33\"][key] = q33\n",
    "    agg_stats[\"q50\"][key] = q50\n",
    "    agg_stats[\"q66\"][key] = q66 \n",
    "    \n",
    "    if len(vals) < min_bucket_size:\n",
    "        continue\n",
    "    \n",
    "    # split tweet into paritions\n",
    "    q50_below, q50_above = [], []\n",
    "    q33_below, q66_above = [], []\n",
    "    \n",
    "    for root_id, val in root_id_val_pairs:\n",
    "        if val >= q50:\n",
    "            q50_above.append(root_id)\n",
    "        else:\n",
    "            q50_below.append(root_id)\n",
    "            \n",
    "        if val >= q66:\n",
    "            q66_above.append(root_id)\n",
    "        elif val <= q33:\n",
    "            q33_below.append(root_id)\n",
    "    \n",
    "    \n",
    "    # downsample to achieve balance\n",
    "    RNG = random.Random(42)\n",
    "    q50_below, q50_above = balance_samples(q50_below, q50_above, RNG)\n",
    "    q33_below, q66_above = balance_samples(q33_below, q66_above, RNG)    \n",
    "    \n",
    "    # print(key, len(q50_above), len(q50_below), \"|\", len(q66_above), len(q33_below))\n",
    "    \n",
    "    paritions[key] = {\n",
    "        \"<q50\": q50_below, \">=q50\": q50_above,\n",
    "        \"<=q33\": q33_below, \">=q66\": q66_above\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3] accumulate labels for each tweet for different prefix / metrics / aggregation\n",
    "\n",
    "# tweet_id1 => {p10__suf_f_tox__tox_bucket__q33_v_q66: True}\n",
    "labels = defaultdict(dict)\n",
    "\n",
    "for prefix in prefixes:\n",
    "    for metric in metrics:\n",
    "        # all\n",
    "        key_p = f\"p{prefix}__{metric}__all\"\n",
    "        key_l = f\"p{prefix}__{metric}__all\"\n",
    "        \n",
    "        # >=q50\n",
    "        for root_id in paritions[key_p][\">=q50\"]:\n",
    "            labels[root_id][f\"{key_l}__>=q50\"] = True\n",
    "\n",
    "        for root_id in paritions[key_p][\"<q50\"]:\n",
    "            labels[root_id][f\"{key_l}__>=q50\"] = False\n",
    "        \n",
    "        # q33_v_q66\n",
    "        for root_id in paritions[key_p][\">=q66\"]:\n",
    "            labels[root_id][f\"{key_l}__q33_v_q66\"] = True\n",
    "\n",
    "        for root_id in paritions[key_p][\"<=q33\"]:\n",
    "            labels[root_id][f\"{key_l}__q33_v_q66\"] = False\n",
    "        \n",
    "        # toxicity buckets\n",
    "        for pre_tox_n in range(p + 1):\n",
    "            key_p = f\"p{prefix}__{metric}__pre_n_tox_{pre_tox_n}\"\n",
    "            key_l = f\"p{prefix}__{metric}__tox_bucket\"\n",
    "            \n",
    "            if key_p not in paritions:\n",
    "                continue\n",
    "\n",
    "            # >=q50\n",
    "            for root_id in paritions[key_p][\">=q50\"]:\n",
    "                labels[root_id][f\"{key_l}__>=q50\"] = True\n",
    "\n",
    "            for root_id in paritions[key_p][\"<q50\"]:\n",
    "                labels[root_id][f\"{key_l}__>=q50\"] = False\n",
    "\n",
    "            # q33_v_q66\n",
    "            for root_id in paritions[key_p][\">=q66\"]:\n",
    "                labels[root_id][f\"{key_l}__q33_v_q66\"] = True\n",
    "\n",
    "            for root_id in paritions[key_p][\"<=q33\"]:\n",
    "                labels[root_id][f\"{key_l}__q33_v_q66\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + other conversation stats\n",
    "out = {}\n",
    "\n",
    "for root_tweet_id, conv_labels in labels.items():\n",
    "    out[root_tweet_id] = {\n",
    "        **ds[root_tweet_id],\n",
    "        **conv_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# output\n",
    "out_fpath = f\"{Config().modeling_dir}/prefix/datasets/{ds_name}_labels.pkl.gz\"\n",
    "\n",
    "with gzip.open(out_fpath, \"wb\") as fout:\n",
    "    pickle.dump(out, fout, protocol=4)\n",
    "    \n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(out.items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_sizes = defaultdict(int)\n",
    "\n",
    "for root_id, conv_labels in labels.items():\n",
    "    conv_dict = ds[root_id]\n",
    "    n = conv_dict[\"n\"]\n",
    "    \n",
    "    for p in prefixes:\n",
    "        for m in metrics:\n",
    "            key_all_q50 = f\"p{p}__{m}__all__>=q50\"\n",
    "            key_all_q33 = f\"p{p}__{m}__all__q33_v_q66\"\n",
    "            key_tox_q50 = f\"p{p}__{m}__tox_bucket__>=q50\"\n",
    "            key_tox_q33 = f\"p{p}__{m}__tox_bucket__q33_v_q66\"\n",
    "            \n",
    "            # (*) size > prefix size\n",
    "            found = False\n",
    "            if (key_all_q50 in conv_labels or \n",
    "                key_all_q33 in conv_labels or \n",
    "                key_tox_q50 in conv_labels or \n",
    "                key_tox_q33 in conv_labels\n",
    "               ):\n",
    "                found = True\n",
    "                # assert n >= 2 * p\n",
    "                assert n >= p + 10\n",
    "            \n",
    "            # (*) if q33_v_q66 = True => >=q50 = True\n",
    "            if key_all_q33 in conv_labels and conv_labels[key_all_q33]:\n",
    "                assert conv_labels.get(key_all_q50, True) == True\n",
    "\n",
    "            if key_tox_q33 in conv_labels and conv_labels[key_tox_q33]:\n",
    "                assert conv_labels.get(key_tox_q50, True) == True\n",
    "            \n",
    "            # (*) check the equalities\n",
    "            if found:\n",
    "                m_val = conv_dict[f\"p{p}_{m}\"]\n",
    "                pre_n_tox = int(conv_dict[f\"p{p}_pre_n_tox\"])\n",
    "                \n",
    "                if key_all_q50 in conv_labels:                    \n",
    "                    if conv_labels[key_all_q50]:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__all\", \">=q50\")] += 1\n",
    "                        assert m_val >= agg_stats[\"q50\"][f\"p{p}__{m}__all\"]\n",
    "                    else:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__all\", \"<q50\")] += 1\n",
    "                        assert m_val < agg_stats[\"q50\"][f\"p{p}__{m}__all\"]\n",
    "                        \n",
    "                if key_all_q33 in conv_labels:\n",
    "                    if conv_labels[key_all_q33]:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__all\", \">=q66\")] += 1\n",
    "                        assert m_val >= agg_stats[\"q66\"][f\"p{p}__{m}__all\"]\n",
    "                    else:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__all\", \"<=q33\")] += 1\n",
    "                        assert m_val <= agg_stats[\"q33\"][f\"p{p}__{m}__all\"]\n",
    "                        \n",
    "                if key_tox_q50 in conv_labels:\n",
    "                    if conv_labels[key_tox_q50]:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\", \">=q50\")] += 1\n",
    "                        assert m_val >= agg_stats[\"q50\"][f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\"]                         \n",
    "                    else:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\", \"<q50\")] += 1\n",
    "                        assert m_val < agg_stats[\"q50\"][f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\"]  \n",
    "                \n",
    "                if key_tox_q33 in conv_labels:\n",
    "                    if conv_labels[key_tox_q33]:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\", \">=q66\")] += 1\n",
    "                        assert m_val >= agg_stats[\"q66\"][f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\"]                         \n",
    "                    else:\n",
    "                        bucket_sizes[(f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\", \"<=q33\")] += 1\n",
    "                        assert m_val <= agg_stats[\"q33\"][f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\"]\n",
    "\n",
    "# (*) are all tweet paritions accounted for\n",
    "for key_qq, count in bucket_sizes.items():\n",
    "    key, qq = key_qq\n",
    "    assert len(paritions[key][qq]) == count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (*) check the aggregate statistics\n",
    "df_list = []\n",
    "\n",
    "for conv in ds.values():\n",
    "    for p in prefixes:\n",
    "        if f\"p{p}_pre_n_tox\" not in conv:\n",
    "            continue\n",
    "            \n",
    "        df_list.append({\n",
    "            \"p\": p,\n",
    "            \"pre_n_tox\": int(conv[f\"p{p}_pre_n_tox\"]),\n",
    "            \"metric\": \"suf_i_tox\",\n",
    "            \"val\": conv[f\"p{p}_suf_i_tox\"]\n",
    "        })\n",
    "        \n",
    "        df_list.append({\n",
    "            \"p\": p,\n",
    "            \"pre_n_tox\": int(conv[f\"p{p}_pre_n_tox\"]),\n",
    "            \"metric\": \"suf_f_tox\",\n",
    "            \"val\": conv[f\"p{p}_suf_f_tox\"]\n",
    "        })\n",
    "        \n",
    "df = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_q33(x):\n",
    "    return np.quantile(x, 1/3)\n",
    "\n",
    "def f_q50(x):\n",
    "    return np.quantile(x, 0.5)\n",
    "\n",
    "def f_q66(x):\n",
    "    return np.quantile(x, 2/3)\n",
    "\n",
    "df_med_all = df\\\n",
    "    .groupby([\"p\", \"metric\"])[\"val\"]\\\n",
    "    .agg([\"count\", f_q33, f_q50, f_q66])\\\n",
    "    .reset_index()\n",
    "\n",
    "df_med_pre = df\\\n",
    "    .groupby([\"p\", \"metric\", \"pre_n_tox\"])[\"val\"]\\\n",
    "    .agg([\"count\", f_q33, f_q50, f_q66])\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df_med_all.iterrows():\n",
    "    p = row[\"p\"]\n",
    "    m = row[\"metric\"]\n",
    "    key = f\"p{p}__{m}__all\"\n",
    "    assert row[\"count\"] == agg_stats[\"n\"][key]    \n",
    "    assert row[\"f_q33\"] == agg_stats[\"q33\"][key]\n",
    "    assert row[\"f_q50\"] == agg_stats[\"q50\"][key]\n",
    "    assert row[\"f_q66\"] == agg_stats[\"q66\"][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df_med_pre.iterrows():\n",
    "    p = row[\"p\"]\n",
    "    m = row[\"metric\"]\n",
    "    pre_n_tox = int(row[\"pre_n_tox\"])\n",
    "    key = f\"p{p}__{m}__pre_n_tox_{pre_n_tox}\"\n",
    "    assert row[\"count\"] == agg_stats[\"n\"][key]    \n",
    "    assert row[\"f_q33\"] == agg_stats[\"q33\"][key]\n",
    "    assert row[\"f_q50\"] == agg_stats[\"q50\"][key]\n",
    "    assert row[\"f_q66\"] == agg_stats[\"q66\"][key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('dtox': virtualenv)",
   "language": "python",
   "name": "python37264bitdtoxvirtualenvbf1c2042b6b64ee8b97dfe3287fe0a24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
